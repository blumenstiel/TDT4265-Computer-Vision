{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1a)\n",
    "We first use the chain rule.\n",
    "\n",
    "$$\\frac{\\partial C^n(w)}{\\partial w_i} = \\frac{\\partial C^n(w)}{\\partial f(x^n)}  \\frac{\\partial f(x^n)}{\\partial w_i}$$\n",
    "\n",
    "$$ \\frac{\\partial C^n(w)}{\\partial w_i} = \\frac{\\partial C^n(w)}{\\partial \\hat{y}^n}  \\frac{\\partial f(x^n)}{\\partial w_i} $$ because $f(x^n) = \\hat{y}^n$\n",
    "\n",
    "Then, we compute $ \\frac{\\partial C^n(w)}{\\partial \\hat{y}^n}$.\n",
    "\n",
    "$$ \\frac{\\partial C^n(w)}{\\partial \\hat{y}^n} = \\frac{\\partial }{\\partial \\hat{y}^n}(-(y^n \\ln(\\hat{y}^n) + (1-y^n) \\ln(1-\\hat{y}^n)))$$\n",
    "\n",
    "$$ \\frac{\\partial C^n(w)}{\\partial \\hat{y}^n} = - (\\frac{y^n}{\\hat{y}^n} + \\frac{y^n - 1}{1 - \\hat{y}^n}) = - (\\frac{y^n - \\hat{y}^n}{\\hat{y}^n(1-\\hat{y}^n)})$$\n",
    "\n",
    "We obtain\n",
    "\n",
    "$$\\frac{\\partial C^n(w)}{\\partial w_i} = -x_i^n f(x^n)(1-f(x^n))(\\frac{y^n - \\hat{y}^n}{\\hat{y}^n(1-\\hat{y}^n)})$$\n",
    "\n",
    "$$\\frac{\\partial C^n(w)}{\\partial w_i} = - (y^n - \\hat{y}^n) x_i^n $$  because $f(x^n) = \\hat{y}^n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1b)\n",
    "We first use the chain rule.\n",
    "\n",
    "$$ \\frac{\\partial C^n(w)}{\\partial w_{ij}} = \\frac{\\partial C^n(w)}{\\partial z_i} \\frac{\\partial z_i}{\\partial w_{ij}} $$\n",
    "\n",
    "$$ \\frac{\\partial C^n(w)}{\\partial z_{i}} = - \\sum \\limits_{k=1}^{K} y_k^n \\frac{\\partial \\ln(\\hat{y}_k^n)}{\\partial z_i} $$\n",
    "\n",
    "$$ \\frac{\\partial C^n(w)}{\\partial z_{i}} = - \\sum \\limits_{k=1}^{K} \\frac{y_k^n}{\\hat{y}_k^n} \\frac{\\partial \\hat{y}_k^n}{\\partial z_i} $$\n",
    "\n",
    "If $k \\ne i$,\n",
    "\n",
    "$$ \\frac{\\partial \\hat{y}_k^n}{\\partial z_i} = \\frac{0 - e^{z_k} e^{z_i}} {(\\sum \\limits_{k'}^{K} e^{z_k'})^2} = - \\hat{y}_k^n \\times \\hat{y}_i^n $$\n",
    "\n",
    "If $k = i$,\n",
    "\n",
    "$$ \\frac{\\partial \\hat{y}_i^n}{\\partial z_i} = \\frac{e^{z_i}\\sum \\limits_{k'}^{K} e^{z_k'} - e^{z_i} e^{z_i}} {(\\sum \\limits_{k'}^{K} e^{z_k'})^2} = \\hat{y}_i^n - (\\hat{y}_i^n)^2 = \\hat{y}_i^n (1 -\\hat{y}_i^n) $$\n",
    "\n",
    "Then,\n",
    "\n",
    "$$ \\frac{\\partial C^n(w)}{\\partial z_{i}} = \\sum \\limits_{k=1,k \\ne i}^{K} \\frac{y_k^n}{\\hat{y}_k^n} \\times \\hat{y}_k^n \\times \\hat{y}_i^n - \\frac{y_i^n}{\\hat{y}_i^n} \\hat{y}_i^n (1 -\\hat{y}_i^n) $$\n",
    "\n",
    "$$ \\frac{\\partial C^n(w)}{\\partial z_{i}} = \\sum \\limits_{k=1,k \\ne i}^{K} y_k^n \\times \\hat{y}_i^n - y_i^n \\times (1-\\hat{y}_i^n) $$\n",
    "\n",
    "$$ \\frac{\\partial C^n(w)}{\\partial z_{i}} = \\sum \\limits_{k=1}^{K} y_k^n \\times \\hat{y}_i^n - y_i^n $$\n",
    "\n",
    "$$ \\frac{\\partial C^n(w)}{\\partial z_{i}} = \\hat{y}_i^n\\sum \\limits_{k=1}^{K} y_k^n - y_i^n  $$\n",
    "\n",
    "$$ \\frac{\\partial C^n(w)}{\\partial z_{i}} = \\hat{y}_i^n - y_i^n $$\n",
    "\n",
    "Finaly,\n",
    "\n",
    "$$ \\frac{\\partial C^n(w)}{\\partial w_{kj}} = \\frac{\\partial C^n(w)}{\\partial z_k} \\frac{\\partial z_k}{\\partial w_{kj}} = (\\hat{y}_k^n - y_k^n) x_j^n = -x_j^n (y_k^n- \\hat{y}_k^n)  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2b)\n",
    "![](task2b_binary_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "![](task2c_binary_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "After how many epochs does early stopping kick in?\n",
    "\n",
    "The training is stopped after 33 epochs in epoch 34 with validation loss of 0.0658."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2e)\n",
    "In the dataset some images are harder to predict than others. In some batches there may be an above-average number of these harder to predict images. If they are not shuffled, they are processed at regular intervals and can be seen in the plot as \"spikes\".\n",
    "\n",
    "If the batches are shuffled, the incorrectly predicted images are distributed more evenly across all batches and there are no negative \"spikes\".\n",
    "![](task2e_train_accuracy_shuffle_difference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3b)\n",
    "![](task3b_softmax_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3c)\n",
    "![](task3b_softmax_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3d)\n",
    "Do you notice any signs of overÔ¨Åtting?\n",
    "\n",
    "Signs of overfitting can be seen. After 3000 training steps, the accuracy of the validation set still increases,\n",
    "but it shows a significantly lower value than the accuracy on the training set. This suggests that the model\n",
    "generalises less well and instead memorises patterns in the training set. This leads to overfitting. However,\n",
    "the increasing validation accuracy also shows that generalised patterns are still being learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{\\partial C(w)}{\\partial w} + \\lambda \\frac{\\partial R(w)}{\\partial w} $$ where $R(w) = ||w||^2 = w^Tw$\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{N} \\sum \\limits_{n=1}^{N} \\frac{\\partial C^n(w)}{\\partial w} + 2 \\lambda w  $$\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = -\\frac{1}{N} \\sum \\limits_{n=1}^{N} x^n(y^n - \\hat{y}^n) + 2 \\lambda w  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "The weights for the model with $\\lambda$ = 1 are less noisy because of the $L_2$ penalty. The penalty forces the weights to be smaller, so that the model is less complex because it is more flexible. That is why the contrast between small weights and high weights is less pronounced than for the model without penalty.\n",
    "\n",
    "![](task4b_softmax_weight.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4c)\n",
    "The validation accuracy degrades when applying any amount\n",
    "of regularization since the regularization lowers the complexity\n",
    "of the neural network model during training, and thus prevent the\n",
    "overfitting. The validation accurancy is then lower during training, but better during testing, because the model is more general.\n",
    "\n",
    "![](task4c_l2_reg_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "![](task4d_l2_reg_norms.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "\n",
    "We observe that the $L_2$ norm is high when the value of $\\lambda$ is low. $L_2$ norm is in inverse proportion to the value of $\\lambda$, which makes sense because the weights are small when the penalty is high."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('py38': conda)",
   "language": "python",
   "name": "python38164bitpy38condac1f68ca5407a4349b0d7e37676f2fbb3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}