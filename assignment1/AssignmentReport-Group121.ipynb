{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report\n",
    "\n",
    "Etienne Gaucher, Benedikt Blumenstiel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1a)\n",
    "We first use the chain rule.\n",
    "\n",
    "$$\\frac{\\partial C^n(w)}{\\partial w_i} = \\frac{\\partial C^n(w)}{\\partial f(x^n)}  \\frac{\\partial f(x^n)}{\\partial w_i}$$\n",
    "\n",
    "$$ \\frac{\\partial C^n(w)}{\\partial w_i} = \\frac{\\partial C^n(w)}{\\partial \\hat{y}^n}  \\frac{\\partial f(x^n)}{\\partial w_i} $$ because $f(x^n) = \\hat{y}^n$\n",
    "\n",
    "Then, we compute $ \\frac{\\partial C^n(w)}{\\partial \\hat{y}^n}$.\n",
    "\n",
    "$$ \\frac{\\partial C^n(w)}{\\partial \\hat{y}^n} = \\frac{\\partial }{\\partial \\hat{y}^n}(-(y^n \\ln(\\hat{y}^n) + (1-y^n) \\ln(1-\\hat{y}^n)))$$\n",
    "\n",
    "$$ \\frac{\\partial C^n(w)}{\\partial \\hat{y}^n} = - (\\frac{y^n}{\\hat{y}^n} + \\frac{y^n - 1}{1 - \\hat{y}^n}) = - (\\frac{y^n - \\hat{y}^n}{\\hat{y}^n(1-\\hat{y}^n)})$$\n",
    "\n",
    "We obtain\n",
    "\n",
    "$$\\frac{\\partial C^n(w)}{\\partial w_i} = -x_i^n f(x^n)(1-f(x^n))(\\frac{y^n - \\hat{y}^n}{\\hat{y}^n(1-\\hat{y}^n)})$$\n",
    "\n",
    "$$\\frac{\\partial C^n(w)}{\\partial w_i} = - (y^n - \\hat{y}^n) x_i^n $$  because $f(x^n) = \\hat{y}^n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1b)\n",
    "We first use the chain rule.\n",
    "\n",
    "$$ \\frac{\\partial C^n(w)}{\\partial w_{ij}} = \\frac{\\partial C^n(w)}{\\partial z_i} \\frac{\\partial z_i}{\\partial w_{ij}} $$\n",
    "\n",
    "$$ \\frac{\\partial C^n(w)}{\\partial z_{i}} = - \\sum \\limits_{k=1}^{K} y_k^n \\frac{\\partial \\ln(\\hat{y}_k^n)}{\\partial z_i} $$\n",
    "\n",
    "$$ \\frac{\\partial C^n(w)}{\\partial z_{i}} = - \\sum \\limits_{k=1}^{K} \\frac{y_k^n}{\\hat{y}_k^n} \\frac{\\partial \\hat{y}_k^n}{\\partial z_i} $$\n",
    "\n",
    "If $k \\ne i$,\n",
    "\n",
    "$$ \\frac{\\partial \\hat{y}_k^n}{\\partial z_i} = \\frac{0 - e^{z_k} e^{z_i}} {(\\sum \\limits_{k'}^{K} e^{z_k'})^2} = - \\hat{y}_k^n \\times \\hat{y}_i^n $$\n",
    "\n",
    "If $k = i$,\n",
    "\n",
    "$$ \\frac{\\partial \\hat{y}_i^n}{\\partial z_i} = \\frac{e^{z_i}\\sum \\limits_{k'}^{K} e^{z_k'} - e^{z_i} e^{z_i}} {(\\sum \\limits_{k'}^{K} e^{z_k'})^2} = \\hat{y}_i^n - (\\hat{y}_i^n)^2 = \\hat{y}_i^n (1 -\\hat{y}_i^n) $$\n",
    "\n",
    "Then,\n",
    "\n",
    "$$ \\frac{\\partial C^n(w)}{\\partial z_{i}} = \\sum \\limits_{k=1,k \\ne i}^{K} \\frac{y_k^n}{\\hat{y}_k^n} \\times \\hat{y}_k^n \\times \\hat{y}_i^n - \\frac{y_i^n}{\\hat{y}_i^n} \\hat{y}_i^n (1 -\\hat{y}_i^n) $$\n",
    "\n",
    "$$ \\frac{\\partial C^n(w)}{\\partial z_{i}} = \\sum \\limits_{k=1,k \\ne i}^{K} y_k^n \\times \\hat{y}_i^n - y_i^n \\times (1-\\hat{y}_i^n) $$\n",
    "\n",
    "$$ \\frac{\\partial C^n(w)}{\\partial z_{i}} = \\sum \\limits_{k=1}^{K} y_k^n \\times \\hat{y}_i^n - y_i^n $$\n",
    "\n",
    "$$ \\frac{\\partial C^n(w)}{\\partial z_{i}} = \\hat{y}_i^n\\sum \\limits_{k=1}^{K} y_k^n - y_i^n  $$\n",
    "\n",
    "$$ \\frac{\\partial C^n(w)}{\\partial z_{i}} = \\hat{y}_i^n - y_i^n $$\n",
    "\n",
    "Finaly,\n",
    "\n",
    "$$ \\frac{\\partial C^n(w)}{\\partial w_{kj}} = \\frac{\\partial C^n(w)}{\\partial z_k} \\frac{\\partial z_k}{\\partial w_{kj}} = (\\hat{y}_k^n - y_k^n) x_j^n = -x_j^n (y_k^n- \\hat{y}_k^n)  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2b)\n",
    "![](task2b_binary_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "![](task2c_binary_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "After how many epochs does early stopping kick in?\n",
    "\n",
    "The training is stopped after 33 epochs in epoch 34 with validation loss of 0.0658."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2e)\n",
    "In the dataset some images are harder to predict than others. In some batches there may be an above-average number of these harder to predict images. If they are not shuffled, they are processed at regular intervals and can be seen in the plot as \"spikes\".\n",
    "\n",
    "If the batches are shuffled, the incorrectly predicted images are distributed more evenly across all batches and there are no negative \"spikes\".\n",
    "![](task2e_train_accuracy_shuffle_difference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3b)\n",
    "![](task3b_softmax_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3c)\n",
    "![](task3b_softmax_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3d)\n",
    "Do you notice any signs of overÔ¨Åtting?\n",
    "\n",
    "Signs of overfitting can be seen. After 2500 training steps, the accuracy of the validation set still increases,\n",
    "but it shows a significantly lower value than the accuracy on the training set. This suggests that the model\n",
    "generalises less well and instead memorises patterns in the training set. This leads to overfitting. However,\n",
    "the increasing validation accuracy also shows that generalised patterns are still being learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{\\partial C(w)}{\\partial w} + \\lambda \\frac{\\partial R(w)}{\\partial w} $$ where $R(w) = ||w||^2 = w^Tw$\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{N} \\sum \\limits_{n=1}^{N} \\frac{\\partial C^n(w)}{\\partial w} + 2 \\lambda w  $$\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = -\\frac{1}{N} \\sum \\limits_{n=1}^{N} x^n(y^n - \\hat{y}^n) + 2 \\lambda w  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "The weights for the model with $\\lambda$ = 1 are less noisy because of the $L_2$ penalty.\n",
    "The penalty forces the weights to be smaller, so that the model is less complex. Other than the model without penalty\n",
    "the model is not overfitting and is learning more general information.\n",
    "\n",
    "![](task4b_softmax_weight.png)\n",
    "\n",
    "Figure 1: The visualization of the weights for a model with $\\lambda$ = 0.0 (top row), and $\\lambda$ = 1.0 (bottom row)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4c)\n",
    "\n",
    "![](task4c_l2_reg_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "\n",
    "The validation accuracy degrades when increasing the amount of regularization since the regularization lowers the\n",
    "complexity of the neural network model during training. Reduced complexity can prevent overfitting.\n",
    "But a high regularization is also limiting the capability of the model to learn general patterns and the model is\n",
    "underfitting. This results in a lower validation accuracy after increasing lambda to 0.1 or even 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "![](task4e_l2_reg_norms.png)\n",
    "We observe that the $L_2$ norm of the weight vector $||w||^2$ is inverse proportional to $\\lambda$.\n",
    "Therefore, the information stored in the weights and the resulting complexity of the model is decreasing with higher\n",
    "$\\lambda$. The low weights with $\\lambda = 1$ leads to underfitting of the model because the penalty of the\n",
    "$L_2$ regularization is too high."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('py38': conda)",
   "language": "python",
   "name": "python38164bitpy38condac1f68ca5407a4349b0d7e37676f2fbb3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}