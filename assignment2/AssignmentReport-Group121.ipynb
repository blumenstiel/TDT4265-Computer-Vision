{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 Report\n",
    "\n",
    "Group 121 â€“ Etienne Gaucher, Benedikt Blumenstiel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task 1a)\n",
    "\n",
    "We first use the chain rule.\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w_{ji}} = \\sum \\limits_{k} \\frac{\\partial C}{\\partial z_k} \\frac{\\partial z_k}{\\partial a_j}\\frac{\\partial a_j}{\\partial z_j}\\frac{\\partial z_j}{\\partial w_{ji}}$$\n",
    "\n",
    "We have $\\frac{\\partial C}{\\partial z_k} = \\delta_k$ by definition, $\\frac{\\partial z_k}{\\partial a_j} = w_{kj}$, $\\frac{\\partial a_j}{\\partial z_j} = f'(z_j)$ and $\\frac{\\partial z_j}{\\partial w_{ji}} = x_i$.\n",
    "\n",
    "We get\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w_{ji}} = \\sum \\limits_{k} \\delta_k w_{kj} f'(z_j)x_i =f'(z_j)x_i \\sum \\limits_{k} \\delta_k w_{kj} $$\n",
    "\n",
    "Thus\n",
    "\n",
    "$$w_{ji} = w_{ji} - \\alpha \\frac{\\partial C}{\\partial w_{ji}} = w_{ji} - \\alpha   \\delta_j x_i$$ with $\\delta_j = f'(z_j) \\sum \\limits_{k}  w_{kj} \\delta_k $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1b)\n",
    "\n",
    "We refer to $W_2$ as the weight matrix from the hidden layer to the output layer, having a shape of $j \\times k$ with\n",
    "$k$ is the number of output neurons and $j$ the number of hidden neurons. Similar, $W_1$ is the weight matrix from\n",
    "the input layer to the hidden layer with a shape of $j \\times i$, where $i$ is the number of input neurons.\n",
    "$\\alpha$ refers to the learning rate.\n",
    "\n",
    "$\\delta_2$ (shape $j \\times k$) is the gradient between hidden layer and output layer, $\\delta_1$ (shape $j \\times i$)\n",
    "is the gradient between input layer and hidden layer. $x$ is the input vector (length $i$), $y$ the target vector\n",
    "(length $k$) and $o$ the predicted output vector (length $k$). $z$ is the output of the hidden layer before activation\n",
    "and $a$ the output of the hidden layer after activation (both length $j$). $f'$ is the derivative sigmoid function.\n",
    "\n",
    "$$W_2 = W_2 - \\alpha \\times \\delta_2$$\n",
    "\n",
    "$$W_1 = W_1 - \\alpha \\times \\delta_1$$\n",
    "\n",
    "\n",
    "$$\\delta_2 = (a)^T \\cdot - (y - o)$$\n",
    "\n",
    "$$\\delta_1 = (X)^T \\cdot - (y - o) \\cdot (W_2)^T \\circ f'(z_1)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "![](task2c_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "\\begin{align*}\n",
    "\\text{Number of parameter} &= \\text{number of weights + number of biases} \\\\\n",
    "&= 785 \\times 64 + 64 \\times 10 + 0 \\\\\n",
    "&= 50 880\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](task3_combined_comparison.png)\n",
    "\n",
    "We can sort all the methods according to learning speed/convergence speed: model 2 < model with all improvements < model\n",
    "with momentum < model with improved weight initialization < model with improved weight initialization and momentum < model\n",
    "with improved weight initialization and sigmoid.\n",
    "\n",
    "The different methods improve a lot the learning speed, but not so much if we use all the tricks at the same time. We do\n",
    "not know the exact explanation of the bad performance, when combining improved sigmoid with momentum. Perhaps there is a\n",
    "overlapping between the methods resulting in bad performance. For example, the improved sigmoid could lead to a very high\n",
    "momentum which leads the model to pass the optimum.\n",
    "\n",
    "We can notice that the use at the same time of improved sigmoid and weight initialization overfit data if we don't stop\n",
    "the learning with early stopping, because the validation cost improves after a certain number of epoch. It is also the case for\n",
    "the model with improved momentum and all the improvements.\n",
    "\n",
    "Regarding the final accuracy, the model with improved weight initialization and momentum has the best score with a\n",
    "accuracy of 0.9643. Therefore, the best models in terms of learning speed and validation loss are the model with\n",
    "improved sigmoid and weight initialization as well as improved weight initialization and momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "![](task4ab_num_hidden_neurons.png)\n",
    "Reducing the number of neurons in the hidden layer to 32 results in a smaller accuracy and higher training loss. This is\n",
    "caused by a smaller capacity of the model (number of training parameters) that is not able to generalize enough.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "The accuracy of the model with 128 hidden neurons is increased compared to the model with 64 hidden neurons, as shown in\n",
    "the previous plot. The increased capacity of the model made it possible to generalize better, but it is also increasing the\n",
    "computation time. Therefore, we have to find a trade-off between increased complexity and improved performance. Increasing\n",
    "the capacity further can also lead to overfitting, which we did not notice in our experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "We are looking for a number of neurons per hidden layer equal to the same number parameter than the previous model.\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Number of parameter} &= \\text{number of weights + number of biases} \\\\\n",
    "50 880 &= 785 \\times x + x \\times + x \\times 10 + 0 \\\\\n",
    "0 &= x^2 + 795x - 50 880\\\\\n",
    "x &= \\frac{795}{2} +   \\sqrt{(\\frac{795}{2})^2 + 50 880}\\\\\n",
    "x &= 59.5407\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, we train the model with two hidden layer with 60 neurons each, resulting in 51 300 parameters.\n",
    "\n",
    "![](task4de_hidden_layers.png)\n",
    "The accuracy of the model with two hidden layers is compareable to the model with one hidden layer. This shows, that\n",
    "in our case the capacity of a model (number of parameters) has a higher influence than the architecture of the model.\n",
    "Apart from the accuracy we notice a lower training loss with two hidden layers. Perhaps the deep structure allow a better\n",
    "optimization of the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "As presented in the previous plot, the model with ten hidden layers has a worse performance compared to models with less\n",
    "layers. The complexity of the model is too high to learn the problem appropriate. This can be caused for example by\n",
    "vanishing gradient. The accuracy and the loss fluctuate a lot, showing a unstable training. The validation loss\n",
    "is also increasing, showing signs of overfitting and results in earnly stopping of the training process. All in all, the\n",
    "high number of hidden layers is not optimal for our implementation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (03_Mask_RCNN_matterport)",
   "language": "python",
   "name": "pycharm-a444bf89"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}